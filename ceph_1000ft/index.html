<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js ie6" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js ie7" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js ie8" lang="en"> <![endif]-->
<!--[if gt IE 8]><!-->  <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
	
	<title>pydown</title>
	
	<meta name="description" content="A jQuery library for modern HTML presentations">
	<meta name="author" content="Caleb Troughton">
	<meta name="viewport" content="width=1024, user-scalable=no">
	
	<!-- Core and extension CSS files -->
	<link rel="stylesheet" href="css/deck.core.css">
	<link rel="stylesheet" href="css/deck.status.css">
	<link rel="stylesheet" href="css/deck.scale.css">
	
	<!-- Style theme. More available in /themes/style/ or create your own. -->
	<link rel="stylesheet" href="css/web-2.0.css">
	
	<!-- Transition theme. More available in /themes/transition/ or create your own. -->
	<link rel="stylesheet" href="css/horizontal-slide.css">

	<link rel="stylesheet" href="css/md_hl.css">
	
	<script src="js/modernizr.custom.js"></script>
</head>

<body class="deck-container">

<!-- Begin slides -->
<section class="slide "><div class="content"><h2>Ceph from 1000ft</h2>
<h3>Danylov Kostiantyn, Mirantis</h3>
<h3>http://koder-ua.blogspot.com/</h3>
<h3>https://github.com/koder-ua/</h3></div></section>
<section class="slide "><div class="content"><h3>Storage</h3>
<ul>
<li>
<p>Free lunch is over (not really)</p>
</li>
<li>
<p>Consistency</p>
</li>
<li>Availability</li>
<li>Partitioning tolerance</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Modern storages - what will you do in case of P</h3>
<ul>
<li>There is no P - NAS (Netapp, EMC, ...)</li>
<li>A - Cassandra<em>, Reak</em>, ...</li>
<li>C - Ceph, Sheepdog, ...</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Ceph from 1000ft</h3>
<ul>
<li>Object storage (Rados)</li>
<li>CRUSH - controled distribution, aware of failure domains</li>
<li>Pools - namespaces with storage settings</li>
<li>RBD, RadosGW, CephFS</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Ceph from 1000ft</h3>
<ul>
<li>Mon + OSD + MDS + RadosGW</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Mon</h3>
<ul>
<li>Keep consistent cluster configuration</li>
<li>Survive nodes failure as long as quorum online</li>
<li>Pass config to OSD/CLients/etc</li>
<li>Store/process no user data</li>
</ul>
<p>Config consists of:</p>
<ul>
<li>CLuster settings</li>
<li>Nodes tree (failure domain aware)</li>
<li>CRUSH</li>
<li>Pool information</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Mon</h3>
<ul>
<li>When no quorum present - cluster became fully unavailable</li>
<li>Creates very moderate load</li>
<li>Requires low IO/CPU/Net latency</li>
<li>Up to ~7 nodes, ususally 3 or 5</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Rados</h3>
<ul>
<li>Reliable Autonomic Distributed Object Store</li>
<li>librados</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>OSD</h3>
<ul>
<li>Store user data; remote access to HDD</li>
<li>1 OSD per 1 HDD</li>
<li>Not need any RAID</li>
<li>LVM should not be used</li>
<li>Responcible for data replication</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>MDS</h3>
<ul>
<li>CephFS metadata storage</li>
<li>Not used in production yet</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>RadosGW</h3>
<ul>
<li>Swift API on top of Rados</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Configuration and logs</h3>
<ul>
<li>/etc/ceph/ceph.conf</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>CLI</h3>
<ul>
<li>rados - pools/objects operations, some very basic performance tests (ones should never use thos tests)</li>
<li>ceph - the rest</li>
<li>rbd - rbd commands</li>
<li>СMD --format json | json_pp | less</li>
<li>СMD -c CONF_FILE -k KEY_FILE</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Authentification + Autorization - CephX</h3>
<ul>
<li>http://docs.ceph.com/docs/master/rados/operations/user-management/</li>
<li>Each user and service has it own key with own previledges</li>
<li>ceph auth list</li>
<li>/etc/ceph/ceph.client.admin.keyring</li>
<li>/var/lib/ceph/bootstrap-osd/</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>OSD tree</h3>
<ul>
<li>ceph osd tree</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>How data stored</h3>
<ul>
<li>File-like</li>
<li>Everything is stored in object (except for monitor data)</li>
<li>Each object has uniq name(in it's pool), data and attributes</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Data storage - CRUSH</h3>
<ul>
<li>Simple programming language to select OSD's using OSD tree</li>
</ul>
<p>ceph osd getcrushmap -o FILE.bin
crushtool -d FILE.bin -o FILE.txt
EDIT FILE.txt
crushtool -c FILE.txt -o FILE_new.bin
ceph osd setcrushmap -i FILE_new.bin</p></div></section>
<section class="slide "><div class="content"><h3>Data storage - CRUSH</h3>
<div class="codehilite"><pre>rule RULENAME {
    ruleset <span class="nt">&lt;ruleset&gt;</span>
    type <span class="cp">[</span> <span class="nx">replicated</span> <span class="o">|</span> <span class="nx">erasure</span> <span class="cp">]</span>
    min_size <span class="nt">&lt;min</span><span class="na">-size</span><span class="nt">&gt;</span>
    max_size <span class="nt">&lt;max</span><span class="na">-size</span><span class="nt">&gt;</span>
    step take <span class="nt">&lt;bucket</span><span class="na">-type</span><span class="nt">&gt;</span>
    step <span class="cp">[</span><span class="nx">choose</span><span class="o">|</span><span class="nx">chooseleaf</span><span class="cp">]</span> <span class="cp">[</span><span class="nx">firstn</span><span class="o">|</span><span class="nx">indep</span><span class="cp">]</span> <span class="nt">&lt;N&gt;</span> <span class="nt">&lt;bucket</span><span class="na">-type</span><span class="nt">&gt;</span>
    step emit
}
</pre></div></div></section>
<section class="slide "><div class="content"><h3>PG</h3>
<p><img alt="PG" src="../images/ceph-data-placement.jpg" /></p></div></section>
<section class="slide "><div class="content"><h3>PG</h3>
<ul>
<li>Optimization, which allows to process objects by groups</li>
<li>Each object beelong to one PG</li>
<li>All object from same PG stored on same set of OSD</li>
<li>More PG leads to more even data distribution and better performance</li>
<li>More PG leads to more RAM and CPU used</li>
<li>More PG leads to longer peering</li>
<li>100-300 PG per OSD (with replication) </li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Pools</h3>
<ul>
<li>Object namespaces</li>
<li>Like folders on FS</li>
<li>Replication, storage and autentification settings</li>
<li>EC</li>
<li>Cache</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Pools CLI</h3>
<ul>
<li>rados lspools / ceph osd lspools</li>
<li>ceph osd dump</li>
<li>rados df</li>
<li>ceph osd df</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Пулы CLI создание</h3>
<ul>
<li>ceph osd pool create {pool-name} {pg-num} [crush-ruleset-name]</li>
</ul></div></section>
<section class="slide "><div class="content"><h3>Пулы CLI mksnap/</h3>
<ul>
<li>ceph osd pool create {pool-name} {pg-num} [crush-ruleset-name]</li>
</ul>
<p>ceph osd map test my-object
ceph pg 4.0 query</p>
<p>$ rados put -p testpool logo.png logo.png
$ ceph osd map testpool logo.png</p></div></section>
<section class="slide "><div class="content"><h3>RBD</h3>
<p><img alt="rbd" src="../images/rbd.jpg" /></p></div></section>
<section class="slide "><div class="content"><h3>RBD</h3>
<ul>
<li>Virtual disk splitted on fixed-size blocks  (powr of 2, 4M by default)</li>
<li>Ech block is mapped to special object with known name </li>
<li>Object, as usually, ar ecreated on first request</li>
<li>http://ceph.com/docs/master/start/quick-rbd/</li>
<li>krbd in kernel may be too old</li>
</ul></div></section>


<!-- deck.status snippet -->
<p class="deck-status">
	<span class="deck-status-current"></span>
	/
	<span class="deck-status-total"></span>
</p>

<!-- Grab CDN jQuery, with a protocol relative URL; fall back to local if offline -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src="js/jquery-1.7.2.min.js"><\/script>')</script>

<!-- Deck Core and extensions -->
<script src="js/deck.core.js"></script>
<script src="js/deck.status.js"></script>
<script src="js/deck.scale.js"></script>

<!-- Initialize the deck -->
<script>
$(function() {
	$.deck('.slide');
});
</script>

</body>
</html>
