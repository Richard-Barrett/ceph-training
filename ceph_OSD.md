!SLIDE
## Устройство OSD 

### Данилов Константин, Mirantis
### http://koder-ua.blogspot.com/
### https://github.com/koder-ua/

!SLIDE
### OSD
* Хранит данные, фактически предоставляя сетевой диступ к диску
* По одному OSD на один диск (Можно несколько на один SSD)
* Заменяют RAID
* LVM только мешает
* Отвечает за передачу данных от и к клиенту и репликацию

!SLIDE
### Хранение данных
* Журнал + основное хранилище
* Все данные пишутся два раза

!SLIDE
### Хранение данных - Журнал
* Небольшой циклический буффер
* Файл или раздел
* Данные в журнал пишутся синхронно
* Клиент не получает подтверждения, пока данные не будут записанны в журналы на всех нодах
* Порча журнала чаще всего невосстановима

!SLIDE
### Хранение данных - Основное хранилище
* Файлы в папках
* Данные пишутся асинхронно
* Иногда сбрасываются на диск
* sudo ceph daemon osd.0 config show | grep filestore
* sudo ceph daemon osd.0 config show | grep filestore_max_sync_interval

!SLIDE
### Хранение данных - Рекомендации
* Журнал на отдельном разделе SSD или на отдельном разделе
* 4-6 журналов на SSD
* Размер журнала 2 * filestore_max_sync_interval * OSD_BW (~10Gb обычно)
* Увеличение filestore_max_sync_interval повышает производительность, но проводит к "подмерзаниям"

!SLIDE
### Хранение данных - Основное хранилище
* Вес OSD
* sudo ceph daemon osd.0 config show | grep osd_data
* sudo ceph daemon osd.0 config show | grep osd_journal
* /var/lib/ceph/osd/ceph-XXX
* /var/lib/ceph/osd/ceph-XXX/journal
* ls -l /var/lib/ceph/osd/ceph-0
* ls -l /var/lib/ceph/osd/ceph-0/current

!SLIDE
### Хранение данных - Основное хранилище
* При старте OSD сканирует папку с данными
* Повторяет незакоммиченные операции из журнала
* Общается с другими OSD, что бы понять что происходило, пока оно лежало
* Возможно реплицирует нехватающие данные

!SLIDE
### Запись данных
* Клиент мапит объект на PG
* Клиент мапит PG+Pool использую crush на набор OSD
* Клиент связывается с первичной OSD и передает ей операцию
* Первичная OSD пишет данные в журнал, в основное хранилище и передает данные в остальные OSD
* При этом используется отдельная репликационная сеть
* OSD отвечают при успешной записи
* Первичная OSD отвечает клиенту
* OSD acting set
* full_ratio, nearfull_ratio

!SLIDE
### Запись данных
* sudo ceph daemon osd.0 config show | grep cluster_network
* sudo ceph daemon osd.0 config show | grep public_network

!SLIDE
### Чтение данных
* Клиент мапит объект на PG
* Клиент мапит PG+Pool использую crush на набор OSD
* Клиент связывается с первичной OSD и она возвращает ему данные

!SLIDE
### Вопросы и помидоры
